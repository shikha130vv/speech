{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3160a235-e527-4429-b3dd-934518f195d1",
   "metadata": {},
   "source": [
    "https://github.com/Azure-Samples/aoai-realtime-audio-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "434ab640-fbe6-49aa-a901-852611cca7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "#from rtclient.models import UserMessageItem, InputTextContentPart, NoTurnDetection, ServerVAD, RTResponse\n",
    "from rtclient import RTClient, RTResponse, ServerVAD, NoTurnDetection, UserMessageItem, InputTextContentPart,InputAudioTranscription\n",
    "import numpy as np\n",
    "import os\n",
    "import sounddevice as sd\n",
    "import base64\n",
    "import time\n",
    "import collections\n",
    "import queue\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "azure_openai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fab5e304-f83a-4cb3-a636-8928ca28687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def simple_text():\n",
    "    # Replace with your Azure key and endpoint\n",
    "    azure_key = azure_openai_api_key\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    "    azure_deployment = azure_openai_deployment\n",
    "    \n",
    "    # Create AzureKeyCredential for authentication\n",
    "    key_credential = AzureKeyCredential(azure_key)\n",
    "\n",
    "    # Instantiate the RTClient\n",
    "    async with RTClient(url=azure_endpoint, key_credential=key_credential, azure_deployment=azure_deployment) as client:\n",
    "        # Configure the client for text modality\n",
    "        await client.configure(modalities={\"text\"}, turn_detection=NoTurnDetection())\n",
    "\n",
    "        while True:\n",
    "            user_input = input(\"You: \")\n",
    "    \n",
    "            # Stop the conversation if the user types \"stop\"\n",
    "            if user_input.lower() == \"stop\":\n",
    "                print(\"Conversation ended.\")\n",
    "                await client.close()\n",
    "                break\n",
    "    \n",
    "            \n",
    "            # Create the text message to send\n",
    "            message = UserMessageItem(\n",
    "                content=[InputTextContentPart(text=user_input)]\n",
    "            )\n",
    "    \n",
    "            # Send the message\n",
    "            await client.send_item(item=message)\n",
    "    \n",
    "            # Generate the response\n",
    "            response = await client.generate_response()\n",
    "    \n",
    "            # Receive the response and handle it\n",
    "            async for item in response:\n",
    "                if item.type == \"message\":\n",
    "                    async for part in item:\n",
    "                        if part.type == \"text\":\n",
    "                            text = \"\"\n",
    "                            async for chunk in part.text_chunks():\n",
    "                                text += chunk\n",
    "                            print(f\"Received text: {text}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f48f7bca-1c5f-4b91-9f27-e2983eb7d376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation ended.\n"
     ]
    }
   ],
   "source": [
    "await simple_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07690a60-ab73-4d95-95d9-d92eb994ea1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_response(client):\n",
    "    while True:\n",
    "        try:\n",
    "            print(\"in process response\")\n",
    "            status = \"\"\n",
    "             \n",
    "            #response = await client.generate_response()\n",
    "            #item = await anext(response)\n",
    "\n",
    "            for _ in [1, 2]:\n",
    "                item = await anext(client.events())\n",
    "                if item.type == \"input_audio\":\n",
    "                    input_item = item\n",
    "                    print(\"input_item\")\n",
    "                    assert input_item is not None\n",
    "                    await input_item\n",
    "                    assert input_item.transcript is not None\n",
    "                    assert len(input_item.transcript) > 0\n",
    "        \n",
    "                if item.type == \"response\":\n",
    "                    response = item\n",
    "                    print(\"response\")\n",
    "\n",
    "\n",
    "\n",
    "            assert response is not None\n",
    "            item = await anext(response)\n",
    "            assert item.type == \"message\"\n",
    "    \n",
    "            print(f\"Response ID: {response.id}\")\n",
    "            print(f\"Response Status: {response.status}\")\n",
    "            print(f\"Response Status Details: {response.status_details}\")\n",
    "            print(f\"Response Output: {response.output}\")\n",
    "            print(f\"Response Usage: {response.usage}\")\n",
    "            if item.type == \"message\":\n",
    "                async for part in item:\n",
    "                    print(part.type)\n",
    "                    if part.type == \"audio\":\n",
    "                        if 1==2:\n",
    "                            async for chunk in part.audio_chunks():\n",
    "                                if len(chunk) % 2 != 0:\n",
    "                                    print(\"Warning: Misaligned buffer size, trimming extra byte.\")\n",
    "                                    chunk = chunk[:-1]\n",
    "                                audio_np = np.frombuffer(chunk, dtype=np.int16)\n",
    "                                add_audio_chunk_to_queue(audio_np)\n",
    "    \n",
    "                        audio_data = b\"\"\n",
    "                        async for chunk in part.audio_chunks():\n",
    "                            audio_data += chunk\n",
    "                    \n",
    "                        if len(audio_data) % 2 != 0:\n",
    "                            print(\"Warning: Misaligned buffer size, trimming extra byte.\")\n",
    "                            audio_data = audio_data[:-1]\n",
    "                    \n",
    "                        audio_np = np.frombuffer(audio_data, dtype=np.int16)\n",
    "                        play_audio(audio_np)\n",
    "                        break\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2764ad69-dda5-414a-98e0-3b3ff9a99b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to record audio from microphone\n",
    "async def record_and_send_audio(client, duration=5, fs=24000):\n",
    "    print(\"Recording audio...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "    sd.wait()  # Wait until recording is finished\n",
    "    print(\"Recording finished.\")\n",
    "    audio =  np.squeeze(audio)\n",
    "    await client.send_audio(audio)\n",
    "    await client.commit_audio()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "human_speaking = False\n",
    "last_speech_time = time.time()\n",
    "pre_roll_buffer = collections.deque(maxlen=20)  # Buffer to store pre-roll audio chunks\n",
    "\n",
    "# Detect speech callback\n",
    "def detect_speech(indata, frames, time1, status):\n",
    "    global human_speaking, last_speech_time\n",
    "    volume_norm = np.linalg.norm(indata) * 10\n",
    "    pre_roll_buffer.append(indata.copy())  # Keep a rolling buffer of pre-speech audio chunks\n",
    "\n",
    "    if volume_norm > 100000:  # Threshold for speech detection\n",
    "        print(\"Speech detected!\")\n",
    "        human_speaking = True  # Mark that speech is happening\n",
    "        last_speech_time = time.time()  # Reset the speech timer\n",
    "    else:\n",
    "        human_speaking = False  # No speech detected\n",
    "\n",
    "# Stream audio, collating and sending in real-time\n",
    "async def record_and_send_audio_streaming(client, fs=24000):\n",
    "    print(\"Recording audio...\")\n",
    "    global human_speaking, last_speech_time\n",
    "    last_speech_time = time.time()\n",
    "\n",
    "    with sd.InputStream(samplerate=fs, channels=1, dtype='int16', callback=detect_speech):\n",
    "        while True:\n",
    "            if human_speaking:\n",
    "                print(\"Sending audio...\")\n",
    "                \n",
    "                # Pre-roll audio: Send chunks stored before speech detection\n",
    "                pre_roll_audio = np.concatenate([np.squeeze(chunk) for chunk in pre_roll_buffer], axis=0)\n",
    "                pre_roll_buffer.clear()  # Clear the pre-roll buffer after sending\n",
    "                \n",
    "                # While the user is speaking, record and send in real-time\n",
    "                while human_speaking:\n",
    "                    audio_chunk = sd.rec(int(0.5 * fs), samplerate=fs, channels=1, dtype='int16')\n",
    "                    sd.wait()  # Wait for the chunk to be fully recorded\n",
    "                    audio_chunk = np.squeeze(audio_chunk)\n",
    "                    \n",
    "                    # Concatenate pre-roll and new audio if pre-roll exists\n",
    "                    if pre_roll_audio.size > 0:\n",
    "                        full_audio = np.concatenate((pre_roll_audio, audio_chunk), axis=0)\n",
    "                        pre_roll_audio = np.array([])  # Clear pre-roll after sending\n",
    "                    else:\n",
    "                        full_audio = audio_chunk\n",
    "                    \n",
    "                    # Send the real-time audio chunk\n",
    "                    await client.send_audio(full_audio)\n",
    "                    await asyncio.sleep(0.001)  # Small delay to avoid overwhelming the system\n",
    "\n",
    "                    if time.time() - last_speech_time > 2:  # Silence threshold (2 seconds)\n",
    "                        print(\"No speech detected for 2 seconds, stopping recording...\")\n",
    "                        break\n",
    "                \n",
    "            else:\n",
    "                # Stop if no speech is detected for more than 5 seconds\n",
    "                if time.time() - last_speech_time > 3:\n",
    "                    print(\"No speech for 3 seconds, stopping...\")\n",
    "                    break\n",
    "\n",
    "            await asyncio.sleep(0.001)  # Wait a little before checking again\n",
    "\n",
    "    # Commit the audio after sending\n",
    "    await client.commit_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "088600e1-b18c-456f-8db1-1cf778391775",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def play_audio_part(part):\n",
    "    audio_data = b\"\"\n",
    "    async for chunk in part.audio_chunks():\n",
    "        audio_data += chunk\n",
    "\n",
    "    if len(audio_data) % 2 != 0:\n",
    "        print(\"Warning: Misaligned buffer size, trimming extra byte.\")\n",
    "        audio_data = audio_data[:-1]\n",
    "\n",
    "    audio_np = np.frombuffer(audio_data, dtype=np.int16)\n",
    "    play_audio(audio_np)\n",
    "\n",
    "async def play_audio_part_streaming(part):\n",
    "    audio_data = b\"\"\n",
    "    async for chunk in part.audio_chunks():\n",
    "        if len(chunk) % 2 != 0:\n",
    "            print(\"Warning: Misaligned buffer size, trimming extra byte.\")\n",
    "            chunk = chunk[:-1]\n",
    "        audio_np = np.frombuffer(chunk, dtype=np.int16)\n",
    "        play_audio(audio_np)\n",
    "\n",
    "# Helper function to play audio output\n",
    "def play_audio(audio):\n",
    "    sd.play(audio, samplerate=24000)\n",
    "    sd.wait()\n",
    "\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "# Callback function to play chunks\n",
    "\n",
    "def audio_callback(outdata, frames, time, status):\n",
    "    if status:\n",
    "        print(status)\n",
    "    try:\n",
    "        audio_chunk = audio_queue.get_nowait()  # Get the next chunk from the queue\n",
    "\n",
    "        # If the chunk is smaller than the buffer size, pad with zeros\n",
    "        if len(audio_chunk) < frames:\n",
    "            audio_chunk = np.pad(audio_chunk, (0, frames - len(audio_chunk)), 'constant')\n",
    "        elif len(audio_chunk) > frames:\n",
    "            audio_chunk = audio_chunk[:frames]\n",
    "\n",
    "        outdata[:] = audio_chunk.reshape(-1, 1)  # Reshape for stereo/mono output\n",
    "    except queue.Empty:\n",
    "        outdata.fill(0)  # If no audio is available, play silence\n",
    "\n",
    "# Function to play audio continuously using callback\n",
    "async def play_audio_continuous(samplerate=24000, channels=1, blocksize=1024):\n",
    "    with sd.OutputStream(samplerate=samplerate, channels=channels, dtype='int16',\n",
    "                         blocksize=blocksize, callback=audio_callback):\n",
    "        while True:\n",
    "            await asyncio.sleep(0.1)  # Keep the loop running\n",
    "\n",
    "# Function to add chunks of audio to the queue\n",
    "def add_audio_chunk_to_queue(audio_chunk):\n",
    "    audio_queue.put(audio_chunk)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e908bb03-2f4f-4070-b220-ea760fcc1cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"\"\n",
    "async def simple_audio():\n",
    "    global user_input\n",
    "    # Replace with your Azure key and endpoint\n",
    "    azure_key = azure_openai_api_key\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    "    azure_deployment = azure_openai_deployment\n",
    "    \n",
    "    # Create AzureKeyCredential for authenticationC\n",
    "    key_credential = AzureKeyCredential(azure_key)\n",
    "\n",
    "    # Instantiate the RTClient\n",
    "    async with RTClient(url=azure_endpoint, key_credential=key_credential, azure_deployment=azure_deployment) as client:\n",
    "        # Configure the client for audio modality\n",
    "        instructions = \"You speak like a person who lives in New Delhi in India in Hindi. You speak in a female voice. You speak 1 sentence in one response. You sound enthusiastic and curious and helpful. \"\n",
    "\n",
    "        await client.configure(modalities={\"audio\",\"text\"}, turn_detection=ServerVAD(), voice=\"alloy\", instructions=instructions,\n",
    "                              input_audio_transcription=InputAudioTranscription(model=\"whisper-1\"))\n",
    "        turn_detection=ServerVAD(),\n",
    "        \n",
    "        while True:\n",
    "            user_input = input(\"Continue?\")\n",
    "            if user_input == \"stop\":\n",
    "                break\n",
    "            try:\n",
    "                await record_and_send_audio_streaming(client)\n",
    "                print(\"here\")\n",
    "                await process_response(client)           \n",
    "            except Exception as e:\n",
    "                await client.close()\n",
    "                print(e)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c1c1b92-69b9-463a-b4a4-e665c3ded1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Continue? hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Speech detected!\n",
      "Sending audio...\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "Speech detected!\n",
      "No speech for 3 seconds, stopping...\n",
      "here\n",
      "in process response\n",
      "input_item\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mCancelledError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m(simple_audio())\n",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m, in \u001b[0;36msimple_audio\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m record_and_send_audio_streaming(client)\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhere\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m process_response(client)           \n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m client\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m, in \u001b[0;36mprocess_response\u001b[1;34m(client)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#response = await client.generate_response()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#item = await anext(response)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m---> 11\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m anext(client\u001b[38;5;241m.\u001b[39mevents())\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_audio\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     13\u001b[0m         input_item \u001b[38;5;241m=\u001b[39m item\n",
      "File \u001b[1;32m~\\FY25\\Tech\\speech\\new_voice_bot\\rtclient\\__init__.py:750\u001b[0m, in \u001b[0;36mRTClient.events\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevents\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[RTInputAudioItem \u001b[38;5;241m|\u001b[39m RTResponse]:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;66;03m# TODO: Add the updated quota message as a control type of event.\u001b[39;00m\n\u001b[0;32m    749\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 750\u001b[0m         message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_message_queue\u001b[38;5;241m.\u001b[39mreceive(\n\u001b[0;32m    751\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m m: m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_audio_buffer.speech_started\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse.created\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    752\u001b[0m         )\n\u001b[0;32m    753\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    754\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\FY25\\Tech\\speech\\new_voice_bot\\rtclient\\util\\message_queue.py:100\u001b[0m, in \u001b[0;36mMessageQueueWithError.receive\u001b[1;34m(self, predicate)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error\n\u001b[1;32m--> 100\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mreceive(\u001b[38;5;28;01mlambda\u001b[39;00m m: predicate(m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_predicate(m))\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error_predicate(message):\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_error \u001b[38;5;241m=\u001b[39m message\n",
      "File \u001b[1;32m~\\FY25\\Tech\\speech\\new_voice_bot\\rtclient\\util\\message_queue.py:82\u001b[0m, in \u001b[0;36mMessageQueue.receive\u001b[1;34m(self, predicate)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_polling \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_task \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_task \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mcreate_task(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll_receive())\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[1;31mCancelledError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "await(simple_audio())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69463536-7b44-477f-9daf-5a9d5103898c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Program interrupted by user (Ctrl+C).\n",
      "Continue?"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main_task = asyncio.create_task(simple_audio())\n",
    "    record_task = asincio.create_task(record_and_send_audio_streaming(client))\n",
    "    #play_task = asyncio.create_task(play_audio_continuous())\n",
    "    await asyncio.gather(main_task, record_task)\n",
    "    #await(simple_audio())\n",
    "except Exception:\n",
    "    print(\"\\nProgram interrupted by user (Ctrl+C).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2d684-4951-47cd-a61c-1158fcd603e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import queue\n",
    "\n",
    "# Create a thread-safe queue to hold audio chunks\n",
    "audio_queue = queue.Queue()\n",
    "\n",
    "def audio_callback(outdata, frames, time, status):\n",
    "    \"\"\"\n",
    "    This callback is called in the background to provide audio chunks to the OutputStream.\n",
    "    It fetches data from the queue and outputs it for playback.\n",
    "    \"\"\"\n",
    "    if status:\n",
    "        print(status)\n",
    "    try:\n",
    "        # Get the next chunk of audio from the queue\n",
    "        audio_chunk = audio_queue.get_nowait()\n",
    "\n",
    "        # Ensure the chunk matches the block size\n",
    "        if len(audio_chunk) < frames:\n",
    "            # If the chunk is smaller than the block size, pad with zeros\n",
    "            audio_chunk = np.pad(audio_chunk, (0, frames - len(audio_chunk)), 'constant')\n",
    "        elif len(audio_chunk) > frames:\n",
    "            # If the chunk is larger, truncate it\n",
    "            audio_chunk = audio_chunk[:frames]\n",
    "\n",
    "        # Reshape to match the output format\n",
    "        outdata[:] = audio_chunk.reshape(-1, 1)\n",
    "\n",
    "    except queue.Empty:\n",
    "        # If no data is available, fill with silence (zeros)\n",
    "        outdata.fill(0)\n",
    "\n",
    "async def play_audio_continuous(samplerate=24000, channels=1, blocksize=1024):\n",
    "    \"\"\"\n",
    "    Start a continuous audio playback stream using audio chunks from a queue.\n",
    "    \"\"\"\n",
    "    with sd.OutputStream(samplerate=samplerate, channels=channels, dtype='int16',\n",
    "                         blocksize=blocksize, callback=audio_callback):\n",
    "        while True:\n",
    "            await asyncio.sleep(0.1)  # Keep the async loop alive while playing audio\n",
    "\n",
    "def add_audio_chunk_to_queue(audio_chunk):\n",
    "    \"\"\"\n",
    "    Add a new audio chunk to the playback queue.\n",
    "    \"\"\"\n",
    "    audio_queue.put(audio_chunk)\n",
    "\n",
    "async def process_audio_events(client):\n",
    "    \"\"\"\n",
    "    Continuously receive audio chunks from the server in real-time and queue them for playback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async for event in client.events():  # Subscribe to the event loop\n",
    "            if isinstance(event, RTResponse):  # If the event is a response, process it\n",
    "                async for item in event:\n",
    "                    if item.type == \"message\":\n",
    "                        async for part in item:\n",
    "                            if part.type == \"audio\":\n",
    "                                # Process audio chunks as they come in\n",
    "                                async for chunk in part.audio_chunks():\n",
    "                                    # Convert the audio chunk to a NumPy array\n",
    "                                    audio_chunk = np.frombuffer(chunk, dtype=np.int16)\n",
    "\n",
    "                                    # Add the audio chunk to the playback queue\n",
    "                                    add_audio_chunk_to_queue(audio_chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during event processing: {e}\")\n",
    "\n",
    "async def low_latency_audio():\n",
    "    \"\"\"\n",
    "    The main function to configure the client, start streaming, and process the audio events.\n",
    "    \"\"\"\n",
    "    # Replace with your Azure key and endpoint\n",
    "    azure_key = azure_openai_api_key\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    "    azure_deployment = azure_openai_deployment\n",
    "    \n",
    "    # Create AzureKeyCredential for authentication\n",
    "    key_credential = AzureKeyCredential(azure_key)\n",
    "\n",
    "    # Instantiate the RTClient\n",
    "    async with RTClient(url=azure_endpoint, key_credential=key_credential, azure_deployment=azure_deployment) as client:\n",
    "        # Configure the client for audio modality\n",
    "        instructions = \"You speak like a person who lives in New Delhi in India in Hindi. You speak in a female voice. You sound enthusiastic and curious and helpful.\"\n",
    "        await client.configure(modalities={\"audio\", \"text\"}, turn_detection=ServerVAD(), voice=\"alloy\", instructions=instructions)\n",
    "\n",
    "        # Create asynchronous tasks for streaming audio and processing events\n",
    "        stream_task = asyncio.create_task(stream_audio(client))\n",
    "        event_task = asyncio.create_task(process_audio_events(client))\n",
    "        playback_task = asyncio.create_task(play_audio_continuous())\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        await asyncio.gather(stream_task, event_task, playback_task)\n",
    "\n",
    "# For Jupyter Notebooks, use this approach instead of asyncio.run()\n",
    "try:\n",
    "    await low_latency_audio()  # Use await directly in a Jupyter Notebook environment\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProgram interrupted by user (Ctrl+C).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346df31-e875-4971-ae3f-73088d0af16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import webrtcvad\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import asyncio\n",
    "import base64\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Helper function to play audio chunk by chunk\n",
    "def play_audio_chunk(pcm_data):\n",
    "    try:\n",
    "        sd.play(pcm_data, samplerate=24000)  # Assuming a 24000 Hz sample rate\n",
    "        sd.wait()\n",
    "    except Exception as e:\n",
    "        print(f\"Error while playing audio chunk: {e}\")\n",
    "\n",
    "# Process the incoming chunk from base64 and play it\n",
    "def process_audio_chunk(chunk):\n",
    "    try:\n",
    "        pcm_data = np.frombuffer(chunk, dtype=np.int16)  # Convert the audio bytes to PCM\n",
    "        play_audio_chunk(pcm_data)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while processing audio chunk: {e}\")\n",
    "\n",
    "async def stream_audio(client, chunk_duration=0.5, fs=24000):\n",
    "    \"\"\"\n",
    "    Streams audio from the microphone in small chunks and sends it to the client.\n",
    "    \n",
    "    Parameters:\n",
    "    chunk_duration (float): The length of each audio chunk in seconds.\n",
    "    fs (int): The sampling rate for audio.\n",
    "    \"\"\"\n",
    "    global human_speaking\n",
    "    buffer_size = int(chunk_duration * fs)  # Calculate buffer size for each chunk in samples\n",
    "    vad = webrtcvad.Vad(0)  # Initialize VAD\n",
    "\n",
    "    # Use an InputStream to capture audio chunk by chunk\n",
    "    with sd.InputStream(samplerate=fs, channels=1, dtype='int16', blocksize=buffer_size, callback=detect_speech) as stream:\n",
    "        print(\"Streaming audio...\")\n",
    "        while True:\n",
    "            try:\n",
    "                # Read a chunk of audio from the microphone\n",
    "                audio_chunk, _ = stream.read(buffer_size)\n",
    "                audio_chunk = np.squeeze(audio_chunk)  # Ensure it's a 1D array\n",
    "\n",
    "                # Use VAD to check if the chunk contains human speec\n",
    "\n",
    "                if human_speaking:\n",
    "                    # Send the audio chunk to the server\n",
    "                    print(\"Speech detected, sending audio chunk\")\n",
    "                    await client.send_audio(audio_chunk.tobytes())\n",
    "                    human_speaking = False\n",
    "                else:\n",
    "                    print(\"No speech detected, skipping chunk\")\n",
    "\n",
    "                # Introduce a short sleep to prevent overwhelming the system\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while streaming audio: {e}\")\n",
    "                break\n",
    "\n",
    "async def process_audio_events(client):\n",
    "    \"\"\"\n",
    "    Continuously receive audio chunks from the server in real-time and play them chunk by chunk.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async for event in client.events():  # Subscribe to the event loop\n",
    "            if isinstance(event, RTResponse):  # If the event is a response, process it\n",
    "                async for item in event:\n",
    "                    if item.type == \"message\":\n",
    "                        async for part in item:\n",
    "                            if part.type == \"audio\":\n",
    "                                # Process audio chunks as they come in\n",
    "                                async for chunk in part.audio_chunks():\n",
    "                                    process_audio_chunk(chunk)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during event processing: {e}\")\n",
    "\n",
    "async def low_latency_audio():\n",
    "    \"\"\"\n",
    "    The main function to configure the client, start streaming and process the audio events.\n",
    "    \"\"\"\n",
    "    # Replace with your Azure key and endpoint\n",
    "    azure_key = azure_openai_api_key\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    "    azure_deployment = azure_openai_deployment\n",
    "    \n",
    "    # Create AzureKeyCredential for authentication\n",
    "    key_credential = AzureKeyCredential(azure_key)\n",
    "\n",
    "    # Instantiate the RTClient\n",
    "    async with RTClient(url=azure_endpoint, key_credential=key_credential, azure_deployment=azure_deployment) as client:\n",
    "        # Configure the client for audio modality\n",
    "        instructions = \"You speak like a person who lives in New Delhi in India in Hindi. You speak in a female voice. You sound enthusiastic and curious and helpful.\"\n",
    "        await client.configure(modalities={\"audio\", \"text\"}, turn_detection=ServerVAD(), voice=\"alloy\", instructions=instructions)\n",
    "\n",
    "        # Create asynchronous tasks for streaming audio and processing events\n",
    "        stream_task = asyncio.create_task(stream_audio(client))\n",
    "        event_task = asyncio.create_task(process_audio_events(client))\n",
    "\n",
    "        # Wait for both tasks to complete (which is effectively \"forever\" unless an error occurs)\n",
    "        await asyncio.gather(stream_task, event_task)\n",
    "\n",
    "# For Jupyter Notebooks, use this approach instead of asyncio.run()\n",
    "try:\n",
    "    await low_latency_audio()  # Use await directly in a Jupyter Notebook environment\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProgram interrupted by user (Ctrl+C).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92309a55-8fc5-42a0-a25b-d15a63ae1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import asyncio\n",
    "\n",
    "# Define a callback to write audio chunks to an open stream\n",
    "def audio_callback(outdata, frames, time, status, pcm_data_queue):\n",
    "    try:\n",
    "        # Get the next chunk of PCM data from the queue\n",
    "        if not pcm_data_queue.empty():\n",
    "            chunk = pcm_data_queue.get_nowait()\n",
    "            # Write the audio chunk to the output stream\n",
    "            outdata[:] = chunk.reshape(outdata.shape)\n",
    "        else:\n",
    "            outdata.fill(0)  # If no audio chunk is available, fill with silence\n",
    "    except Exception as e:\n",
    "        print(f\"Error in audio callback: {e}\")\n",
    "        outdata.fill(0)  # On error, fill with silence\n",
    "\n",
    "# Initialize a queue to hold PCM audio data\n",
    "pcm_data_queue = asyncio.Queue()\n",
    "\n",
    "# Function to play audio chunks via a non-blocking stream\n",
    "async def play_audio_stream(fs=24000, channels=1):\n",
    "    \"\"\"\n",
    "    Play audio in a non-blocking stream. Continuously pulls chunks from the queue and plays them.\n",
    "    \"\"\"\n",
    "    with sd.OutputStream(samplerate=fs, channels=channels, dtype='int16', callback=lambda outdata, frames, time, status: audio_callback(outdata, frames, time, status, pcm_data_queue)):\n",
    "        print(\"Audio stream is playing...\")\n",
    "        await asyncio.Event().wait()  # Keep the stream open forever or until canceled\n",
    "\n",
    "# Function to process the incoming base64-encoded audio chunk\n",
    "async def process_audio_chunk(chunk):\n",
    "    \n",
    "\n",
    "    # Convert the bytes to 16-bit signed integers (int16)\n",
    "    pcm_data = np.frombuffer(chunk, dtype=np.int16)\n",
    "\n",
    "    # Put the PCM data into the queue for playback\n",
    "    await pcm_data_queue.put(pcm_data)\n",
    "\n",
    "# Function to handle real-time audio events from the client\n",
    "async def process_audio_events(client):\n",
    "    \"\"\"\n",
    "    Continuously receive audio chunks from the server in real-time and enqueue them for playback.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        async for event in client.events():  # Subscribe to the event loop\n",
    "            if isinstance(event, RTResponse):  # If the event is a response, process it\n",
    "                async for item in event:\n",
    "                    if item.type == \"message\":\n",
    "                        async for part in item:\n",
    "                            if part.type == \"audio\":\n",
    "                                # Process audio chunks as they come in\n",
    "                                async for chunk in part.audio_chunks():\n",
    "                                    await process_audio_chunk(chunk)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during event processing: {e}\")\n",
    "\n",
    "# Function to handle audio streaming (recording) and sending it to the client\n",
    "async def stream_audio(client, chunk_duration=0.5, fs=24000):\n",
    "    \"\"\"\n",
    "    Streams audio from the microphone in small chunks and sends it to the client.\n",
    "    \"\"\"\n",
    "    buffer_size = int(chunk_duration * fs)  # Calculate buffer size for each chunk in samples\n",
    "    \n",
    "    # Use an InputStream to capture audio chunk by chunk\n",
    "    with sd.InputStream(samplerate=fs, channels=1, dtype='int16', blocksize=buffer_size) as stream:\n",
    "        print(\"Streaming audio...\")\n",
    "        while True:\n",
    "            try:\n",
    "                # Read a chunk of audio from the microphone\n",
    "                audio_chunk, _ = stream.read(buffer_size)\n",
    "                audio_chunk = np.squeeze(audio_chunk)\n",
    "\n",
    "                # Send the audio chunk to the server\n",
    "                await client.send_audio(audio_chunk.tobytes())\n",
    "\n",
    "                # Introduce a short sleep to prevent overwhelming the system\n",
    "                await asyncio.sleep(0.1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error while streaming audio: {e}\")\n",
    "                break\n",
    "\n",
    "# Main function to initialize and run the low-latency audio system\n",
    "async def low_latency_audio():\n",
    "    \"\"\"\n",
    "    The main function to configure the client, start streaming and process the audio events.\n",
    "    \"\"\"\n",
    "    # Replace with your Azure key and endpoint\n",
    "    azure_key = azure_openai_api_key\n",
    "    azure_endpoint = azure_openai_endpoint\n",
    "    azure_deployment = azure_openai_deployment\n",
    "    \n",
    "    # Create AzureKeyCredential for authentication\n",
    "    key_credential = AzureKeyCredential(azure_key)\n",
    "\n",
    "    # Instantiate the RTClient\n",
    "    async with RTClient(url=azure_endpoint, key_credential=key_credential, azure_deployment=azure_deployment) as client:\n",
    "        # Configure the client for audio modality\n",
    "        instructions = \"You speak like a person who lives in New Delhi in India in Hindi. You speak in a female voice. You sound enthusiastic and curious and helpful.\"\n",
    "        await client.configure(modalities={\"audio\", \"text\"}, turn_detection=ServerVAD(), voice=\"alloy\", instructions=instructions)\n",
    "\n",
    "        # Create asynchronous tasks for streaming audio and processing events\n",
    "        stream_task = asyncio.create_task(stream_audio(client))\n",
    "        play_audio_task = asyncio.create_task(play_audio_stream())  # Play the audio stream\n",
    "        event_task = asyncio.create_task(process_audio_events(client))\n",
    "\n",
    "        # Wait for both tasks to complete (which is effectively \"forever\" unless an error occurs)\n",
    "        await asyncio.gather(stream_task, event_task, play_audio_task)\n",
    "\n",
    "# For Jupyter Notebooks, use this approach instead of asyncio.run()\n",
    "try:\n",
    "    await low_latency_audio()  # Use await directly in a Jupyter Notebook environment\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nProgram interrupted by user (Ctrl+C).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd81bbe-f54a-4136-98da-4a235724c088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
