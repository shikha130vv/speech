{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/Azure-Samples/aoai-realtime-audio-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install load-dotenv\n",
    "#!pip install pyaudio\n",
    "#!pip install gradio\n",
    "# git clone https://github.com/Azure-Samples/aoai-realtime-audio-sdk.git\n",
    "# copy aoai-realtime-audio-sdk/python/rtclient .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "import os\n",
    "import requests\n",
    "import asyncio\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "from player import Player\n",
    "from recorder import Recorder\n",
    "from assistants import AssistantService\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "from rtclient import RealtimeException, RTClient, RTInputAudioItem, RTResponse\n",
    "from rtclient.models import InputAudioTranscription, InputTextContentPart, NoTurnDetection, ServerVAD, UserMessageItem\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "#from rtclient import RTClient\n",
    "\n",
    "\n",
    "import rtclient.low_level_client as llc\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "realtime_client = None\n",
    "audio_recorder = None\n",
    "audio_player = None\n",
    "assistant_service = AssistantService()\n",
    "\n",
    "recording_active = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rt_client():\n",
    "    azure_openai_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    azure_openai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    azure_openai_deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "    # Create an RT client\n",
    "    realtime_client = RTClient(\n",
    "        url=azure_openai_endpoint,\n",
    "        key_credential=AzureKeyCredential(azure_openai_key),\n",
    "        azure_deployment=azure_openai_deployment,\n",
    "    )\n",
    "    return realtime_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def process_audio_recording_buffer(base64_audio):\n",
    "    if recording_active:\n",
    "        await realtime_streaming.send({\n",
    "            \"type\": \"input_audio_buffer.append\",\n",
    "            \"audio\": base64_audio,\n",
    "        })\n",
    "\n",
    "\n",
    "async def reset_audio(start_recording, language, temperature, voice):\n",
    "    global recording_active, audio_recorder, audio_player\n",
    "\n",
    "    recording_active = False\n",
    "    if audio_recorder:\n",
    "        audio_recorder.stop()\n",
    "    if audio_player:\n",
    "        audio_player.clear()\n",
    "\n",
    "    audio_recorder = Recorder(process_audio_recording_buffer)\n",
    "    audio_player = Player()\n",
    "    audio_player.init(24000)\n",
    "\n",
    "    if start_recording:\n",
    "        stream = await get_audio_stream()  # Replace with your implementation to fetch an audio stream\n",
    "        await audio_recorder.start(stream)\n",
    "        recording_active = True\n",
    "\n",
    "\n",
    "def is_azure_openai():\n",
    "    return False  # Set this to True if using Azure OpenAI\n",
    "\n",
    "\n",
    "def get_audio_stream():\n",
    "    # Mock function to simulate getting an audio stream\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def stop_audio():\n",
    "    global realtime_streaming\n",
    "    recording_active = False\n",
    "    audio_recorder.stop()\n",
    "    audio_player.clear()\n",
    "    realtime_streaming.close()\n",
    "    return \"Audio and streaming stopped.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "async def handle_text_input(user_input):  \n",
    "    global realtime_client\n",
    "    await realtime_client.configure(modalities={\"text\"}, turn_detection=NoTurnDetection())\n",
    "    await realtime_client.send_item(\n",
    "        item=UserMessageItem(\n",
    "            content=[InputTextContentPart(text=user_input)]\n",
    "        )\n",
    "    )\n",
    "    response = await realtime_client.generate_response()\n",
    "\n",
    "    item = await anext(response)\n",
    "    assert item.type == \"message\"\n",
    "    \n",
    "    async for part in item:\n",
    "        text = \"\"\n",
    "        assert part.type == \"text\"\n",
    "        async for chunk in part.text_chunks():\n",
    "            assert chunk is not None\n",
    "            text += chunk\n",
    "        assert part.text == text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(language, temperature, voice):\n",
    "    # Start the session with the assistant and handle audio interaction\n",
    "    asyncio.run(start_realtime(endpoint, api_key, deployment, language, temperature, voice))\n",
    "    return \"Real-time session started.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "async def start_realtime(endpoint, api_key, deployment, language, temperature, voice):\n",
    "    try:\n",
    "        global realtime_client\n",
    "    \n",
    "        # Simulating initialization of the real-time stream client\n",
    "        realtime_client = create_rt_client()\n",
    "        # Code to create rt client\n",
    "    \n",
    "        await realtime_client.connect()\n",
    "        \n",
    "        assistant_service.language = language\n",
    "        assistant = assistant_service.create_generic_assistant_config_message()\n",
    "        \n",
    "        await realtime_client.configure(\n",
    "                   turn_detection=ServerVAD(),\n",
    "                   input_audio_transcription=InputAudioTranscription(model=\"whisper-1\"),\n",
    "                   tools=assistant[1],\n",
    "                   voice=voice,\n",
    "                   instructions=assistant[0],\n",
    "                   temperature=temperature\n",
    "                )\n",
    "        \n",
    "        return \"Session started successfully!\"\n",
    "    except Exception as error:\n",
    "        return f\"Error: Unable to start session. Details: {error}\"\n",
    "\n",
    "\n",
    "async def handle_realtime_messages():\n",
    "    global realtime_streaming\n",
    "\n",
    "    async for message in realtime_streaming.messages():\n",
    "        message = json.loads(message)  # Assuming message is received in JSON format\n",
    "\n",
    "        # Handle different message types, similar to the original JS code\n",
    "        if message['type'] == 'session.created':\n",
    "            print(f\"Session Created: {json.dumps(message, indent=2)}\")\n",
    "            # Handle session state or UI changes here\n",
    "\n",
    "        elif message['type'] == 'conversation.item.created':\n",
    "            # Handle user messages being created\n",
    "            if message['item']['type'] == \"message\" and message['item']['role'] == \"user\":\n",
    "                append_message_id(message['item']['id'])  # Custom function to append the message ID\n",
    "\n",
    "        elif message['type'] == 'response.content_part.added':\n",
    "            # Append the assistant's response to the UI\n",
    "            append_to_text_block(\"Assistant: \")\n",
    "\n",
    "        elif message['type'] == 'response.audio_transcript.delta':\n",
    "            # Append transcription data\n",
    "            append_to_text_block(message['delta'])\n",
    "\n",
    "        elif message['type'] == 'response.audio.delta':\n",
    "            # Handle binary audio data and play with Player\n",
    "            binary_data = base64.b64decode(message['delta'])\n",
    "            pcm_data = np.frombuffer(binary_data, dtype=np.int16)\n",
    "            audio_player.play(pcm_data)\n",
    "\n",
    "        elif message['type'] == 'input_audio_buffer.speech_started':\n",
    "            # Handle audio recording started\n",
    "            append_to_text_block(\"\")  # Start a new line for the next audio input\n",
    "            audio_player.clear()\n",
    "\n",
    "        elif message['type'] == 'conversation.item.input_audio_transcription.completed':\n",
    "            # Append the completed transcription\n",
    "            append_to_text_block(f\"User (Speech): {message['transcript']} >> {message['item_id']}\")\n",
    "\n",
    "        elif message['type'] == 'response.done':\n",
    "            # Handle when the assistant's response is fully processed\n",
    "            for output in message['response']['output']:\n",
    "                if output['type'] == 'function_call':\n",
    "                    response = await assistant_service.get_tool_response(output['name'], output['arguments'], output['call_id'])\n",
    "                    if response['type'] == 'session.update':\n",
    "                        response['session']['voice'] = voice\n",
    "                        response['session']['temperature'] = temperature\n",
    "                    await realtime_streaming.send(json.dumps(response))\n",
    "                    await realtime_streaming.send(json.dumps({\"type\": \"response.create\"}))\n",
    "                elif output['type'] == 'message':\n",
    "                    append_message_id(output['id'])\n",
    "\n",
    "        elif message['type'] == 'error':\n",
    "            # Handle any errors received from the real-time streaming\n",
    "            print(f\"Error: {json.dumps(message, indent=2)}\")\n",
    "\n",
    "    # Reset audio once the session ends\n",
    "    await reset_audio(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7866\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7866/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id=None type='session.update' session=SessionUpdateParams(model=None, modalities={'text'}, voice=None, instructions=None, input_audio_format=None, output_audio_format=None, input_audio_transcription=None, turn_detection=NoTurnDetection(type='none'), tools=None, tool_choice=None, temperature=None, max_response_output_tokens=None)\n",
      "{\"type\":\"session.update\",\"session\":{\"modalities\":[\"text\"],\"turn_detection\":{\"type\":\"none\"}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shagrawal\\Miniconda3\\envs\\rtep\\Lib\\site-packages\\gradio\\queueing.py\", line 622, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shagrawal\\Miniconda3\\envs\\rtep\\Lib\\site-packages\\gradio\\route_utils.py\", line 323, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shagrawal\\Miniconda3\\envs\\rtep\\Lib\\site-packages\\gradio\\blocks.py\", line 2014, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shagrawal\\Miniconda3\\envs\\rtep\\Lib\\site-packages\\gradio\\blocks.py\", line 1565, in call_function\n",
      "    prediction = await fn(*processed_input)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shagrawal\\Miniconda3\\envs\\rtep\\Lib\\site-packages\\gradio\\utils.py\", line 813, in async_wrapper\n",
      "    response = await f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\shagrawal\\AppData\\Local\\Temp\\ipykernel_31584\\3360205409.py\", line 3, in handle_text_input\n",
      "    await realtime_client.configure(modalities={\"text\"}, turn_detection=NoTurnDetection())\n",
      "  File \"C:\\Users\\shagrawal\\FY25\\Tech\\speech\\new_voice_bot\\rtclient\\__init__.py\", line 685, in configure\n",
      "    if message.type == \"error\":\n",
      "       ^^^^^^^^^^^^\n",
      "AttributeError: 'NoneType' object has no attribute 'type'\n"
     ]
    }
   ],
   "source": [
    "# Gradio UI for the app\n",
    "gradio_interface = gr.Blocks()\n",
    "\n",
    "with gradio_interface:\n",
    "    gr.Markdown(\"# Real-Time Assistant Interaction\")\n",
    "\n",
    "    with gr.Row():\n",
    "        language_input = gr.Textbox(label=\"Language\", value=\"English\")\n",
    "        temperature_input = gr.Slider(0.0, 1.0, step=0.1, label=\"Temperature\", value=0.7)\n",
    "        voice_input = gr.Dropdown(choices=[\"alloy\", \"echo\", \"shimmer\"], label=\"Voice\", value=\"alloy\")\n",
    "\n",
    "    start_button = gr.Button(\"Start Real-Time Session\")\n",
    "    stop_button = gr.Button(\"Stop Audio and Streaming\")\n",
    "\n",
    "    user_input = gr.Textbox(label=\"Your Input\")\n",
    "    send_button = gr.Button(\"Send Message\")\n",
    "\n",
    "    output = gr.Textbox(label=\"Output\")\n",
    "\n",
    "    start_button.click(fn=main, inputs=[language_input, temperature_input, voice_input], outputs=output)\n",
    "    stop_button.click(fn=stop_audio, outputs=output)\n",
    "    send_button.click(fn=handle_text_input, inputs=user_input, outputs=output)\n",
    "\n",
    "gradio_interface.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<rtclient.RTClient at 0x1c53b156950>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realtime_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'session.update', 'session': {'turn_detection': {'type': 'server_vad'}, 'instructions': '\\n        ##Role\\n        You are an expert, well-training agent for support center.\\n        You are a native speaker of English without any accents.\\n        Use function calling to switch to specialized assistant.\\n        ', 'tools': [{'type': 'function', 'name': 'get_weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'location for the weather'}}}, 'description': 'get the weather of the location'}, {'type': 'function', 'name': 'Assistant_MobileAssistant', 'parameters': {'type': 'object', 'properties': {}}, 'description': 'Help user to answer mobile related questions, such as billing, contract, etc.'}, {'type': 'function', 'name': 'Assistant_ShopAssistant', 'parameters': {'type': 'object', 'properties': {}}, 'description': 'Help user to answer shop-related questions, such as shop location, available time, etc.'}, {'type': 'function', 'name': 'search_generic_information', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'query'}}}, 'description': 'Use this function to search for generic information. Do not tell customers fake billing information before getting a result.'}]}}\n",
      "event_id=None type='session.update' session=SessionUpdateParams(model=None, modalities=None, voice='alloy', instructions='\\n        ##Role\\n        You are an expert, well-training agent for support center.\\n        You are a native speaker of English without any accents.\\n        Use function calling to switch to specialized assistant.\\n        ', input_audio_format=None, output_audio_format=None, input_audio_transcription=InputAudioTranscription(model='whisper-1'), turn_detection=ServerVAD(type='server_vad', threshold=None, prefix_padding_ms=None, silence_duration_ms=None), tools=[{'type': 'function', 'name': 'get_weather', 'parameters': {'type': 'object', 'properties': {'location': {'type': 'string', 'description': 'location for the weather'}}}, 'description': 'get the weather of the location'}, {'type': 'function', 'name': 'Assistant_MobileAssistant', 'parameters': {'type': 'object', 'properties': {}}, 'description': 'Help user to answer mobile related questions, such as billing, contract, etc.'}, {'type': 'function', 'name': 'Assistant_ShopAssistant', 'parameters': {'type': 'object', 'properties': {}}, 'description': 'Help user to answer shop-related questions, such as shop location, available time, etc.'}, {'type': 'function', 'name': 'search_generic_information', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'query'}}}, 'description': 'Use this function to search for generic information. Do not tell customers fake billing information before getting a result.'}], tool_choice=None, temperature=0.7, max_response_output_tokens=None)\n",
      "{\"type\":\"session.update\",\"session\":{\"voice\":\"alloy\",\"instructions\":\"\\n        ##Role\\n        You are an expert, well-training agent for support center.\\n        You are a native speaker of English without any accents.\\n        Use function calling to switch to specialized assistant.\\n        \",\"input_audio_transcription\":{\"model\":\"whisper-1\"},\"turn_detection\":{\"type\":\"server_vad\"},\"tools\":[{\"type\":\"function\",\"name\":\"get_weather\",\"parameters\":{\"type\":\"object\",\"properties\":{\"location\":{\"type\":\"string\",\"description\":\"location for the weather\"}}},\"description\":\"get the weather of the location\"},{\"type\":\"function\",\"name\":\"Assistant_MobileAssistant\",\"parameters\":{\"type\":\"object\",\"properties\":{}},\"description\":\"Help user to answer mobile related questions, such as billing, contract, etc.\"},{\"type\":\"function\",\"name\":\"Assistant_ShopAssistant\",\"parameters\":{\"type\":\"object\",\"properties\":{}},\"description\":\"Help user to answer shop-related questions, such as shop location, available time, etc.\"},{\"type\":\"function\",\"name\":\"search_generic_information\",\"parameters\":{\"type\":\"object\",\"properties\":{\"query\":{\"type\":\"string\",\"description\":\"query\"}}},\"description\":\"Use this function to search for generic information. Do not tell customers fake billing information before getting a result.\"}],\"temperature\":0.7}}\n",
      "event_id=None type='session.update' session=SessionUpdateParams(model=None, modalities={'text'}, voice=None, instructions=None, input_audio_format=None, output_audio_format=None, input_audio_transcription=None, turn_detection=NoTurnDetection(type='none'), tools=None, tool_choice=None, temperature=None, max_response_output_tokens=None)\n",
      "{\"type\":\"session.update\",\"session\":{\"modalities\":[\"text\"],\"turn_detection\":{\"type\":\"none\"}}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  hi\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id=None type='conversation.item.create' previous_item_id=None item=UserMessageItem(type='message', role='user', id='item-GxasQWWu8dYhbHJOcOm0fbeGmAN', content=[InputTextContentPart(type='input_text', text='hi')], status=None)\n",
      "{\"type\":\"conversation.item.create\",\"previous_item_id\":null,\"item\":{\"type\":\"message\",\"role\":\"user\",\"id\":\"item-GxasQWWu8dYhbHJOcOm0fbeGmAN\",\"content\":[{\"type\":\"input_text\",\"text\":\"hi\"}]}}\n",
      "event_id=None type='response.create' response=None\n",
      "{\"type\":\"response.create\"}\n",
      "Hello! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  how are you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id=None type='conversation.item.create' previous_item_id=None item=UserMessageItem(type='message', role='user', id='item-qQvNBKFh0EUn-hN9BXPaoYo6xTA', content=[InputTextContentPart(type='input_text', text='how are you')], status=None)\n",
      "{\"type\":\"conversation.item.create\",\"previous_item_id\":null,\"item\":{\"type\":\"message\",\"role\":\"user\",\"id\":\"item-qQvNBKFh0EUn-hN9BXPaoYo6xTA\",\"content\":[{\"type\":\"input_text\",\"text\":\"how are you\"}]}}\n",
      "event_id=None type='response.create' response=None\n",
      "{\"type\":\"response.create\"}\n",
      "\n",
      "Hello! How can I help you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  How is weather in gurgaon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id=None type='conversation.item.create' previous_item_id=None item=UserMessageItem(type='message', role='user', id='item-5UWUWBfHdRL_mqy6HcGXKvfBavw', content=[InputTextContentPart(type='input_text', text='How is weather in gurgaon')], status=None)\n",
      "{\"type\":\"conversation.item.create\",\"previous_item_id\":null,\"item\":{\"type\":\"message\",\"role\":\"user\",\"id\":\"item-5UWUWBfHdRL_mqy6HcGXKvfBavw\",\"content\":[{\"type\":\"input_text\",\"text\":\"How is weather in gurgaon\"}]}}\n",
      "event_id=None type='response.create' response=None\n",
      "{\"type\":\"response.create\"}\n",
      "I'm doing well, thank you! How about you?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter something:  what is weather in gurgaon\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event_id=None type='conversation.item.create' previous_item_id=None item=UserMessageItem(type='message', role='user', id='item-zM7MeQ2gEmqXCceBuh7bn1TerGO', content=[InputTextContentPart(type='input_text', text='what is weather in gurgaon')], status=None)\n",
      "{\"type\":\"conversation.item.create\",\"previous_item_id\":null,\"item\":{\"type\":\"message\",\"role\":\"user\",\"id\":\"item-zM7MeQ2gEmqXCceBuh7bn1TerGO\",\"content\":[{\"type\":\"input_text\",\"text\":\"what is weather in gurgaon\"}]}}\n",
      "event_id=None type='response.create' response=None\n",
      "{\"type\":\"response.create\"}\n",
      "\n",
      "I'm doing well, thank you! How about you?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Simulating initialization of the real-time stream client\n",
    "realtime_client = create_rt_client()\n",
    "# Code to create rt client\n",
    "\n",
    "await realtime_client.connect()\n",
    "\n",
    "assistant_service.language = \"English\"\n",
    "assistant = assistant_service.create_generic_assistant_config_message()\n",
    "print(assistant)\n",
    "await realtime_client.configure(\n",
    "           turn_detection=ServerVAD(),\n",
    "           input_audio_transcription=InputAudioTranscription(model=\"whisper-1\"),\n",
    "           tools=assistant[\"session\"][\"tools\"],\n",
    "           voice=\"alloy\",\n",
    "           instructions=assistant[\"session\"][\"instructions\"],\n",
    "           temperature=0.7\n",
    "        )\n",
    "\n",
    "\n",
    "await realtime_client.configure(modalities={\"text\"}, turn_detection=NoTurnDetection())\n",
    "\n",
    "user_input = input(\"Enter something: \")\n",
    "while user_input != \"stop\":\n",
    "    await realtime_client.send_item(\n",
    "            item=UserMessageItem(\n",
    "                content=[InputTextContentPart(text=user_input)]\n",
    "            )\n",
    "        )\n",
    "    response = await realtime_client.generate_response()\n",
    "    try:\n",
    "        item = await anext(response)\n",
    "        assert item.type == \"message\"\n",
    "        \n",
    "        async for part in item:\n",
    "            text = \"\"\n",
    "            assert part.type == \"text\"\n",
    "            async for chunk in part.text_chunks():\n",
    "                assert chunk is not None\n",
    "                text += chunk\n",
    "            assert part.text == text\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(text)\n",
    "    user_input = input(\"Enter something: \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
